{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import bs4\n",
    "import math\n",
    "import requests\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlalchemy as db\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the constants\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "WEATHER_CSV = [\"2009_weather.csv\", \"2010_weather.csv\", \"2011_weather.csv\", \"2012_weather.csv\", \"2013_weather.csv\", \"2014_weather.csv\", \"2015_weather.csv\"]\n",
    "\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculating distance\n",
    "\n",
    "The most important thing in a trip is the distance, so we need to calculate the distance from the start to the end of the trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(from_coord, to_coord):\n",
    "    \"\"\"\n",
    "    Calculate the distance between two coordinates in kilometers.\n",
    "    args:\n",
    "        from_coord: dataframe of columns 'pickup_latitude' and 'pickup_longitude'\n",
    "        to_coord: dataframe of columns 'dropoff_latitude' and 'dropoff_longitude'\n",
    "    returns:\n",
    "        distance in kilometers\n",
    "    \"\"\"\n",
    "    pickup_lat = from_coord['pickup_latitude'].map(math.radians)\n",
    "    pickup_lon = from_coord['pickup_longitude'].map(math.radians)\n",
    "    dropoff_lat = to_coord['dropoff_latitude'].map(math.radians)\n",
    "    dropoff_lon = to_coord['dropoff_longitude'].map(math.radians)\n",
    "    R = 6378.137 # Radius of earth in KM\n",
    "    a = pickup_lat - dropoff_lat\n",
    "    b = pickup_lon - dropoff_lon\n",
    "    c = ((a / 2).map(math.sin)) ** 2 + pickup_lat.map(math.cos) * dropoff_lat.map(math.cos) * ((b / 2).map(math.sin) ** 2)\n",
    "    distance = 2 * R * c.map(math.sqrt).map(math.asin)\n",
    "\n",
    "    return distance.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(dataframe):\n",
    "    '''\n",
    "    Adds a column to the dataframe with the distance between the pickup and dropoff coordinates.\n",
    "    args:\n",
    "        dataframe: a pandas dataframe with columns \"pickup_latitude\" , \"pickup_longitude\",  \"dropoff_latitude\" and \"dropoff_longitude\"\n",
    "    returns:\n",
    "        a copy of the dataframe with an additional column \"distance\" containing the distance between the pickup and dropoff coordinates\n",
    "    '''\n",
    "\n",
    "    from_coord = dataframe[['pickup_latitude', 'pickup_longitude']]\n",
    "    to_coord = dataframe[['dropoff_latitude', 'dropoff_longitude']]\n",
    "    dataframe['distance'] = calculate_distance(from_coord, to_coord)\n",
    "\n",
    "    # remove any rows where the distance is bigger than 100 km\n",
    "    dataframe = dataframe[dataframe['distance'] < 100]\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Processing Taxi Data\n",
    "\n",
    "We will load and clean the yellow taxi data from 2009-01 to 2009-06."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_taxi_parquet_urls():\n",
    "    '''\n",
    "    Finds the urls of the parquet files for the taxi data.\n",
    "    returns:\n",
    "        a list of urls\n",
    "    '''\n",
    "    strhtml = requests.get(TAXI_URL)\n",
    "    soup = bs4.BeautifulSoup(strhtml.text, \"html.parser\")\n",
    "    links = soup.find_all(\"a\", href=True)\n",
    "\n",
    "    urls = []\n",
    "    for link in links:\n",
    "        # use regular expression to find the links that match the pattern 2009-01~2015-06\n",
    "        if re.search(r\"yellow_tripdata_2009|yellow_tripdata_201[0-4]|yellow_tripdata_2015-0[1-6]\", link.get(\"href\")):\n",
    "            urls.append(link.get(\"href\"))\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo = gpd.read_file(filename='taxi_zones.zip', engine='fiona')\n",
    "geo = geo.to_crs(4326)\n",
    "# get the coordinates of the pickup and dropoff locations\n",
    "geo['longitude'] = geo.centroid.x\n",
    "geo['latitude'] = geo.centroid.y\n",
    "# some location IDs are duplicated so those IDs are removed \n",
    "geo.drop_duplicates('LocationID',inplace=True)\n",
    "geo.set_index('LocationID',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month_taxi_data(url):\n",
    "    '''\n",
    "    Downloads the taxi data for a given month and cleans it.\n",
    "    args:\n",
    "        url: the url of the parquet file\n",
    "    returns:\n",
    "        a pandas dataframe with the cleaned data\n",
    "    '''\n",
    "    file_name = url.split('/')[-1]\n",
    "\n",
    "    # download the file if it doesn't exist\n",
    "    if not os.path.exists(file_name):\n",
    "        print(\"downloading\", file_name)\n",
    "        file = requests.get(url)\n",
    "        with open(file_name , \"wb\") as f:\n",
    "            f.write(file.content)\n",
    "\n",
    "    df = pd.read_parquet(file_name)\n",
    "    print(\"cleaning\", file_name)\n",
    "\n",
    "    # looking up the latitude and longitude for some months where only location IDs are given for pickups and dropoffs\n",
    "    if 'PULocationID' in df.columns:\n",
    "        df['pickup_latitude'] = df['PULocationID'].map(geo['latitude'])\n",
    "        df['pickup_longitude'] = df['PULocationID'].map(geo['longitude'])\n",
    "        df['dropoff_latitude'] = df['DOLocationID'].map(geo['latitude'])\n",
    "        df['dropoff_longitude'] = df['DOLocationID'].map(geo['longitude'])\n",
    "\n",
    "    # normalizing column names\n",
    "    df.rename(columns={'tpep_pickup_datetime':'pickup_datetime','Trip_Pickup_DateTime':'pickup_datetime','Trip_Dropoff_DateTime':'dropoff_datetime','Start_Lon':'pickup_longitude', 'Start_Lat':'pickup_latitude', 'End_Lon':'dropoff_longitude', 'End_Lat':'dropoff_latitude','Tip_Amt':'tip_amount'}, inplace=True)\n",
    "\n",
    "    # some location IDs are valid so those specific trips are removed \n",
    "    df.dropna(subset=['pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude'],inplace=True)\n",
    "\n",
    "    # removing unnecessary columns\n",
    "    df = df[['PULocationID','DOLocationID','pickup_datetime','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','tip_amount']]\n",
    "\n",
    "    # removing invalid data points\n",
    "    df = df[df['tip_amount'] >= 0]\n",
    "\n",
    "    # removing trips that start and/or end outside of the NEW_YORK_BOX_COORDS\n",
    "    df = df[(df['pickup_latitude'] >= NEW_YORK_BOX_COORDS[0][0]) & (df['pickup_latitude'] <= NEW_YORK_BOX_COORDS[1][0])]\n",
    "    df = df[(df['pickup_longitude'] >= NEW_YORK_BOX_COORDS[0][1]) & (df['pickup_longitude'] <= NEW_YORK_BOX_COORDS[1][1])]\n",
    "\n",
    "    # sample the data to make it roughly equal to the size of the Uber dataset\n",
    "    # ~200000 is the number of rows in the Uber dataset\n",
    "    # so every month we sample 200000 / 78 = 2564 rows\n",
    "    df = df.sample(n=2564)\n",
    "\n",
    "    # normalizing and using appropriate column types for the respective data\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "    df = df.astype({'PULocationID':'int8','DOLocationID':'int8','pickup_longitude':'float32','pickup_latitude':'float32','dropoff_longitude':'float32','dropoff_latitude':'float32','tip_amount':'float32'})\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceee168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data():\n",
    "    '''\n",
    "    Downloads the taxi data and cleans it.\n",
    "    returns:\n",
    "        a pandas dataframe with the cleaned taxi data\n",
    "    '''\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    all_parquet_urls = find_taxi_parquet_urls()\n",
    "    for parquet_url in all_parquet_urls:\n",
    "        dataframe = get_and_clean_month_taxi_data(parquet_url)\n",
    "        add_distance_column(dataframe)\n",
    "\n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data\n",
    "\n",
    "Now we should process the Uber data. We should make sure that the data is in the same format as the taxi data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "    '''\n",
    "    Loads the Uber data and cleans it.\n",
    "    args:\n",
    "        csv_file: the csv file with the Uber data\n",
    "    returns:\n",
    "        a pandas dataframe with the cleaned Uber data\n",
    "    '''\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print(\"cleaning\", csv_file)\n",
    "    \n",
    "    # removing unnecessary columns\n",
    "    df = df[['pickup_datetime','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude']]\n",
    "\n",
    "    # removing trips that start and/or end outside of the NEW_YORK_BOX_COORDS\n",
    "    df = df[(df['pickup_latitude'] >= NEW_YORK_BOX_COORDS[0][0]) & (df['pickup_latitude'] <= NEW_YORK_BOX_COORDS[1][0])]\n",
    "    df = df[(df['pickup_longitude'] >= NEW_YORK_BOX_COORDS[0][1]) & (df['pickup_longitude'] <= NEW_YORK_BOX_COORDS[1][1])]\n",
    "\n",
    "    # normalizing and using appropriate column types for the respective data\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "    df = df.astype({'pickup_longitude':'float32','pickup_latitude':'float32','dropoff_longitude':'float32','dropoff_latitude':'float32'})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    '''\n",
    "    Load Uber data and cleans it.\n",
    "    returns:\n",
    "        a pandas dataframe with the cleaned Uber data\n",
    "    '''\n",
    "    if os.path.exists(UBER_CSV.replace('.csv','_cleaned.csv')):\n",
    "        print(\"reading\", UBER_CSV.replace('.csv','_cleaned.csv'))\n",
    "        return pd.read_csv(UBER_CSV.replace('.csv','_cleaned.csv'))\n",
    "\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_CSV)\n",
    "    add_distance_column(uber_dataframe)\n",
    "    \n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data\n",
    "\n",
    "We will load and clean the weather data from 2009-01 to 2009-06."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    '''\n",
    "    Loads the weather data and cleans it.\n",
    "    args:\n",
    "        csv_file: the csv file with the weather data\n",
    "    returns:\n",
    "        a pandas dataframe with the cleaned weather data\n",
    "    '''\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # removing unnecessary columns\n",
    "    df = df[['DATE','HourlyWindSpeed','HourlyPrecipitation']]\n",
    "    \n",
    "    # normalizing and using appropriate column types for the respective data\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df['HourlyPrecipitation'] = pd.to_numeric(df['HourlyPrecipitation'], errors='coerce')\n",
    "\n",
    "    # fill in missing values\n",
    "    mean = int(df['HourlyWindSpeed'].mean())\n",
    "    df['HourlyWindSpeed'].fillna(mean, inplace=True)\n",
    "    df['HourlyPrecipitation'].fillna(0, inplace=True)\n",
    "\n",
    "    df = df.astype({'HourlyWindSpeed': 'int8', 'HourlyPrecipitation': 'float64'})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    '''\n",
    "    Loads the weather data and groups it by day.\n",
    "    args:\n",
    "        csv_file: the csv file with the weather data\n",
    "    returns:\n",
    "        a pandas dataframe with the cleaned weather data\n",
    "    '''\n",
    "    df = clean_month_weather_data_hourly(csv_file)\n",
    "\n",
    "    # group by hour and take the mean of the windspeed and sum of the precipitation\n",
    "    df['DATE'] = df['DATE'].dt.date\n",
    "    df = df.groupby('DATE').agg({'HourlyWindSpeed': np.mean, 'HourlyPrecipitation': np.sum})\n",
    "\n",
    "    # normalizing and using appropriate column types for the respective data\n",
    "    df.reset_index(inplace=True)\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df.rename(columns={'HourlyWindSpeed': 'DailyWindSpeed', 'HourlyPrecipitation': 'DailyPrecipitation'}, inplace=True)\n",
    "    df = df.astype({'DailyWindSpeed':'int8','DailyPrecipitation':'float64'})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    '''\n",
    "    Loads the weather data and cleans it.\n",
    "    returns:\n",
    "        a pandas dataframe with the cleaned weather data\n",
    "    '''\n",
    "\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "\n",
    "    for csv_file in WEATHER_CSV:\n",
    "        print(\"cleaning\", csv_file)\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "\n",
    "    # create two dataframes with data from every month needed\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "\n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f900f7aa",
   "metadata": {},
   "source": [
    "### Process All Data\n",
    "\n",
    "Now we can process all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data = get_and_clean_taxi_data()\n",
    "uber_data = get_uber_data()\n",
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data\n",
    "\n",
    "_Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4753fcd",
   "metadata": {},
   "source": [
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] For 01-2009 through 06-2015, what hour of the day was the most popular to take a yellow taxi? The result should have 24 bins.\n",
    "* [ ] For the same time frame, what day of the week was the most popular to take an uber? The result should have 7 bins.\n",
    "* [ ] What is the 95% percentile of distance traveled for all hired trips during July 2013?\n",
    "* [ ] What were the top 10 days with the highest number of hired rides for 2009, and what was the average distance for each day?\n",
    "* [ ] Which 10 days in 2014 were the windiest, and how many hired trips were made on those days?\n",
    "* [ ] During Hurricane Sandy in NYC (Oct 29-30, 2012) and the week leading up to it, how many trips were taken each hour, and for each hour, how much precipitation did NYC receive and what was the sustained wind speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query N\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._\n",
    "\n",
    "_Repeat for each query_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_N = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_N).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_N, \"some_descriptive_name.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data\n",
    "\n",
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] Create an appropriate visualization for the first query/question in part 3\n",
    "* [ ] Create a visualization that shows the average distance traveled per month (regardless of year - so group by each month). Include the 90% confidence interval around the mean in the visualization\n",
    "* [ ] Define three lat/long coordinate boxes around the three major New York airports: LGA, JFK, and EWR (you can use bboxfinder to help). Create a visualization that compares what day of the week was most popular for drop offs for each airport.\n",
    "* [ ] Create a heatmap of all hired trips over a map of the area. Consider using KeplerGL or another library that helps generate geospatial visualizations.\n",
    "* [ ] Create a scatter plot that compares tip amount versus distance.\n",
    "* [ ] Create another scatter plot that compares tip amount versus precipitation amount.\n",
    "\n",
    "_Be sure these cells are executed so that the visualizations are rendered when the notebook is submitted._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization N\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._\n",
    "\n",
    "_Repeat for each visualization._\n",
    "\n",
    "_The example below makes use of the `matplotlib` library. There are other libraries, including `pandas` built-in plotting library, kepler for geospatial data representation, `seaborn`, and others._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_n(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_n():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_n()\n",
    "plot_visual_n(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc50670726e829d05772f0a8941686a10215a3e79c3aeb954cea988b6ae2af0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
