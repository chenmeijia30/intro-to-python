{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import bs4\n",
    "import math\n",
    "import requests\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sqlalchemy as db\n",
    "import geopandas as gpd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "WEATHER_CSV = [\"2009_weather.csv\", \"2010_weather.csv\", \"2011_weather.csv\", \"2012_weather.csv\", \"2013_weather.csv\", \"2014_weather.csv\", \"2015_weather.csv\"]\n",
    "\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Taxi Zone Shapefile to get the GEOmetry of each zone\n",
    "GEO = gpd.read_file(filename='taxi_zones.zip', engine='fiona')\n",
    "GEO = GEO.to_crs(4326)\n",
    "\n",
    "# get the coordinates of the pickup and dropoff locations\n",
    "GEO['longitude'] = GEO.centroid.x\n",
    "GEO['latitude'] = GEO.centroid.y\n",
    "\n",
    "# some location IDs are duplicated so those IDs are removed \n",
    "GEO.drop_duplicates('LocationID',inplace=True)\n",
    "GEO.set_index('LocationID',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing\n",
    "\n",
    "In Part1, we will preprocess the data. We will do the following steps:\n",
    "\n",
    "1. Calculate the distance using latitude and longitude.\n",
    "2. Download the yellow_tripdata_yyyy-mm.parquet from the NYC Taxi & Limousine Commission using `requests` library.\n",
    "3. Load and clean the yellow taxi data from 2009-01 to 2015-06.\n",
    "4. Load and clean the uber data.\n",
    "5. Load and clean the weather data from 2009 to 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculating distance\n",
    "\n",
    "The most important thing in a trip is the distance, so we need to calculate the distance from the start to the end of the trip. Considering the earth is a sphere, we can use the Haversine formula to calculate the distance. The formula is:\n",
    "\n",
    "$$\n",
    "d = 2r\\arcsin\\left(\\sqrt{\\sin^2\\left(\\frac{\\phi_2-\\phi_1}{2}\\right)+\\cos\\phi_1\\cos\\phi_2\\sin^2\\left(\\frac{\\lambda_2-\\lambda_1}{2}\\right)}\\right)\n",
    "$$\n",
    "\n",
    "The $r$ is the radius of the earth, which is 6371 km. $\\phi$ is the latitude, and $\\lambda$ is the longitude. The formula is from [Wikipedia](https://en.wikipedia.org/wiki/Haversine_formula)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(from_coord: pd.core.frame.DataFrame, to_coord: pd.core.frame.DataFrame) -> pd.core.series.Series:\n",
    "    \"\"\"Calculate the distance between two coordinates in kilometers.\n",
    "\n",
    "    Keyword arguments:\n",
    "    from_coord -- A dataframe of columns 'pickup_latitude' and 'pickup_longitude'\n",
    "    to_coord -- A dataframe of columns 'dropoff_latitude' and 'dropoff_longitude'\n",
    "\n",
    "    Returns:\n",
    "    A series of the distance between the two coordinates in kilometers.\n",
    "    \"\"\"\n",
    "\n",
    "    # convert degree to radian\n",
    "    pickup_lat = from_coord['pickup_latitude'].map(math.radians)\n",
    "    pickup_lon = from_coord['pickup_longitude'].map(math.radians)\n",
    "    dropoff_lat = to_coord['dropoff_latitude'].map(math.radians)\n",
    "    dropoff_lon = to_coord['dropoff_longitude'].map(math.radians)\n",
    "\n",
    "    # calculate the distance\n",
    "    R = 6371 # Radius of earth in KM\n",
    "    a = pickup_lat - dropoff_lat\n",
    "    b = pickup_lon - dropoff_lon\n",
    "    c = ((a / 2).map(math.sin)) ** 2 + pickup_lat.map(math.cos) * dropoff_lat.map(math.cos) * ((b / 2).map(math.sin) ** 2)\n",
    "    distance = 2 * R * c.map(math.sqrt).map(math.asin)\n",
    "    \n",
    "    return distance.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(df: pd.core.frame.DataFrame) -> pd.core.frame.DataFrame:\n",
    "    '''Add a column to the dataframe with the distance between the pickup and dropoff coordinates.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    df -- A dataframe with columns \"pickup_latitude\" , \"pickup_longitude\",  \"dropoff_latitude\" and \"dropoff_longitude\".\n",
    "\n",
    "    Returns:\n",
    "    A copy of the dataframe with an additional column \"distance\" containing the distance between the pickup and dropoff coordinates\n",
    "    '''\n",
    "\n",
    "    from_coord = df[['pickup_latitude', 'pickup_longitude']]\n",
    "    to_coord = df[['dropoff_latitude', 'dropoff_longitude']]\n",
    "    df['distance'] = calculate_distance(from_coord, to_coord)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Processing Taxi Data\n",
    "\n",
    "We will download, load and clean the yellow taxi data from 2009-01 to 2009-06. \n",
    "\n",
    "At first, we should get the urls to download the data, we can use `requests`, `BeautifulSoup` and `re` to get the urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_taxi_parquet_urls() -> list:\n",
    "    '''Finds the urls of the parquet files for the taxi data.\n",
    "\n",
    "    Returns:\n",
    "    A list of urls that point to the parquet files.\n",
    "    '''\n",
    "\n",
    "    strhtml = requests.get(TAXI_URL)\n",
    "    soup = bs4.BeautifulSoup(strhtml.text, \"html.parser\")\n",
    "    links = soup.find_all(\"a\", href=True)\n",
    "\n",
    "    urls = []\n",
    "    for link in links:\n",
    "        # use regular expression to find the links that match the pattern 2009-01~2015-06\n",
    "        if re.search(r\"yellow_tripdata_2009|yellow_tripdata_201[0-4]|yellow_tripdata_2015-0[1-6]\", link.get(\"href\")):\n",
    "            urls.append(link.get(\"href\"))\n",
    "\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ba35ad",
   "metadata": {},
   "source": [
    "Second, we should download the data from url and save it. Then we can load the data and clean it. We mainly use `pandas` to load and clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month_taxi_data(url: str) -> pd.core.frame.DataFrame:\n",
    "    '''Download,load and clean the taxi data for a given month.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    url -- The url of the parquet file.\n",
    "\n",
    "    Returns:\n",
    "    A pandas dataframe with the cleaned data.\n",
    "    '''\n",
    "\n",
    "    # get file name from url\n",
    "    file_name = url.split('/')[-1]\n",
    "\n",
    "    # download the file if it doesn't exist\n",
    "    if not os.path.exists(file_name):\n",
    "        print(\"downloading\", file_name)\n",
    "        file = requests.get(url)\n",
    "        with open(file_name , \"wb\") as f:\n",
    "            f.write(file.content)\n",
    "\n",
    "    # load the data\n",
    "    df = pd.read_parquet(file_name)\n",
    "    print(\"cleaning\", file_name)\n",
    "\n",
    "    # looking up the latitude and longitude for some months where only location IDs are given for pickups and dropoffs\n",
    "    if 'PULocationID' in df.columns:\n",
    "        df['pickup_latitude'] = df['PULocationID'].map(GEO['latitude'])\n",
    "        df['pickup_longitude'] = df['PULocationID'].map(GEO['longitude'])\n",
    "        df['dropoff_latitude'] = df['DOLocationID'].map(GEO['latitude'])\n",
    "        df['dropoff_longitude'] = df['DOLocationID'].map(GEO['longitude'])\n",
    "\n",
    "    # normalizing column names\n",
    "    df.rename(columns={'tpep_pickup_datetime':'pickup_datetime','Trip_Pickup_DateTime':'pickup_datetime','Trip_Dropoff_DateTime':'dropoff_datetime','Start_Lon':'pickup_longitude', 'Start_Lat':'pickup_latitude', 'End_Lon':'dropoff_longitude', 'End_Lat':'dropoff_latitude','Tip_Amt':'tip_amount'}, inplace=True)\n",
    "\n",
    "    # some location IDs are invalid so those specific trips are removed \n",
    "    df.dropna(subset=['pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude'],inplace=True)\n",
    "\n",
    "    # removing invalid data points\n",
    "    df = df[(df['tip_amount'] >= 0)]\n",
    "\n",
    "    # removing trips that start and/or end outside of the NEW_YORK_BOX_COORDS\n",
    "    df = df[(df['pickup_latitude'] >= NEW_YORK_BOX_COORDS[0][0]) & (df['pickup_latitude'] <= NEW_YORK_BOX_COORDS[1][0])]\n",
    "    df = df[(df['pickup_longitude'] >= NEW_YORK_BOX_COORDS[0][1]) & (df['pickup_longitude'] <= NEW_YORK_BOX_COORDS[1][1])]\n",
    "    df = df[(df['dropoff_latitude'] >= NEW_YORK_BOX_COORDS[0][0]) & (df['dropoff_latitude'] <= NEW_YORK_BOX_COORDS[1][0])]\n",
    "    df = df[(df['dropoff_longitude'] >= NEW_YORK_BOX_COORDS[0][1]) & (df['dropoff_longitude'] <= NEW_YORK_BOX_COORDS[1][1])]\n",
    "\n",
    "    # removing unnecessary columns\n",
    "    df = df[['pickup_datetime','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','tip_amount']]\n",
    "\n",
    "    # sample the data to make it roughly equal to the size of the Uber dataset\n",
    "    # ~200000 is the number of rows in the Uber dataset\n",
    "    # so every month we sample 200000 / 78 = 2564 rows\n",
    "    df = df.sample(n=2564)\n",
    "\n",
    "    # normalizing and using appropriate column types for the respective data\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "    df = df.astype({'pickup_longitude':'float32','pickup_latitude':'float32','dropoff_longitude':'float32','dropoff_latitude':'float32','tip_amount':'float32'})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceee168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(parquet_urls: list) -> pd.core.frame.DataFrame:\n",
    "    '''Downloads the taxi data and cleans it.\n",
    "\n",
    "    Returns:\n",
    "    A pandas dataframe with the sampled, cleaned taxi data.\n",
    "    '''\n",
    "\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    # loop through all the urls\n",
    "    for parquet_url in parquet_urls:\n",
    "        dataframe = get_and_clean_month_taxi_data(parquet_url)\n",
    "        add_distance_column(dataframe)\n",
    "\n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data\n",
    "\n",
    "Now we process the Uber data. We should make sure that the data is in the same format as the taxi data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data() -> pd.core.frame.DataFrame:\n",
    "    '''Loads the Uber data and cleans it.\n",
    "\n",
    "    Returns:\n",
    "    A pandas dataframe with the cleaned Uber data.\n",
    "    '''\n",
    "\n",
    "    # load the data\n",
    "    df = pd.read_csv(UBER_CSV)\n",
    "    print(\"cleaning\", UBER_CSV)\n",
    "    \n",
    "    # removing unnecessary columns\n",
    "    df = df[['pickup_datetime','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude']]\n",
    "\n",
    "    # removing trips that start and/or end outside of the NEW_YORK_BOX_COORDS\n",
    "    df = df[(df['pickup_latitude'] >= NEW_YORK_BOX_COORDS[0][0]) & (df['pickup_latitude'] <= NEW_YORK_BOX_COORDS[1][0])]\n",
    "    df = df[(df['pickup_longitude'] >= NEW_YORK_BOX_COORDS[0][1]) & (df['pickup_longitude'] <= NEW_YORK_BOX_COORDS[1][1])]\n",
    "    df = df[(df['dropoff_latitude'] >= NEW_YORK_BOX_COORDS[0][0]) & (df['dropoff_latitude'] <= NEW_YORK_BOX_COORDS[1][0])]\n",
    "    df = df[(df['dropoff_longitude'] >= NEW_YORK_BOX_COORDS[0][1]) & (df['dropoff_longitude'] <= NEW_YORK_BOX_COORDS[1][1])]\n",
    "\n",
    "    # normalizing and using appropriate column types for the respective data\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "    df = df.astype({'pickup_longitude':'float32','pickup_latitude':'float32','dropoff_longitude':'float32','dropoff_latitude':'float32'})\n",
    "\n",
    "    # add the distance column\n",
    "    add_distance_column(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data\n",
    "\n",
    "Now, we will load and clean the weather data from 2009 to 2015. \n",
    "\n",
    "First, we load the hourly weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file: str) -> pd.core.frame.DataFrame:\n",
    "    '''Loads the weather data and cleans it.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    csv_file -- The csv file with the weather data.\n",
    "\n",
    "    Returns:\n",
    "    A pandas dataframe with the cleaned hourly weather data\n",
    "    '''\n",
    "\n",
    "    # load the data\n",
    "    df = pd.read_csv(csv_file, low_memory=False)\n",
    "    \n",
    "    # remove unnecessary columns\n",
    "    df = df[['DATE','HourlyWindSpeed','HourlyPrecipitation']]\n",
    "\n",
    "    # remove missing values\n",
    "    df.dropna(subset=['HourlyWindSpeed'], inplace=True)\n",
    "\n",
    "    # normalize and use appropriate column types for the respective data\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df['HourlyPrecipitation'] = pd.to_numeric(df['HourlyPrecipitation'], errors='coerce')\n",
    "\n",
    "    # fill in missing values\n",
    "    df['HourlyPrecipitation'].fillna(0, inplace=True)\n",
    "\n",
    "    df = df.astype({'HourlyWindSpeed': 'float32', 'HourlyPrecipitation': 'float32'})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd95343d",
   "metadata": {},
   "source": [
    "Then, we load and clean the daily weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file: str) -> tuple[pd.core.frame.DataFrame, pd.core.frame.DataFrame]:\n",
    "    '''Loads the weather data and groups it by day.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    csv_file: the csv file with the weather data\n",
    "\n",
    "    Returns:\n",
    "    A pandas dataframe with the cleaned weather data\n",
    "    '''\n",
    "\n",
    "    # load the data\n",
    "    df = pd.read_csv(csv_file, low_memory=False)\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df['HourlyPrecipitation'] = pd.to_numeric(df['HourlyPrecipitation'], errors='coerce')\n",
    "    df['HourlyPrecipitation'].fillna(0, inplace=True)\n",
    "\n",
    "    # group by hour and take the mean of the windspeed and sum of the precipitation\n",
    "    df['DATE'] = df['DATE'].dt.date\n",
    "    df = df.groupby('DATE', as_index=False).agg({'HourlyWindSpeed': np.mean, 'HourlyPrecipitation': np.sum, 'Sunrise': 'first', 'Sunset': 'first'})\n",
    "    df['HourlyWindSpeed'] = df['HourlyWindSpeed'].map(lambda x: round(x, 2))\n",
    "\n",
    "    # remove unnecessary columns\n",
    "    df_sun = df[['DATE','Sunrise','Sunset']].copy()\n",
    "    df_sun = df_sun.dropna()\n",
    "    df = df[['DATE','HourlyWindSpeed','HourlyPrecipitation']]\n",
    "\n",
    "    # normalize and use appropriate column types for the respective data\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df_sun['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df.rename(columns={'HourlyWindSpeed': 'DailyWindSpeed', 'HourlyPrecipitation': 'DailyPrecipitation'}, inplace=True)\n",
    "\n",
    "    df_sun = df_sun.astype({'Sunrise': 'int32', 'Sunset': 'int32'})\n",
    "    df = df.astype({'DailyWindSpeed':'float32', 'DailyPrecipitation':'float32'})\n",
    "    \n",
    "    return df, df_sun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    '''Loads the weather data and cleans it.\n",
    "\n",
    "    Returns:\n",
    "    A dataframe with the cleaned weather data.\n",
    "    '''\n",
    "\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "    sun_dataframes = []\n",
    "\n",
    "    for csv_file in WEATHER_CSV:\n",
    "        print(\"cleaning\", csv_file)\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframe, daily_sun = clean_month_weather_data_daily(csv_file)\n",
    "        sun_dataframes.append(daily_sun)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "\n",
    "    # create three dataframes with data from every month needed\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    sun_data = pd.concat(sun_dataframes)\n",
    "\n",
    "    return hourly_data, daily_data, sun_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f900f7aa",
   "metadata": {},
   "source": [
    "### Process All Data\n",
    "\n",
    "After defining all the functions, we can process all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_parquet_urls = find_taxi_parquet_urls()\n",
    "taxi_data = get_and_clean_taxi_data(all_parquet_urls)\n",
    "uber_data = load_and_clean_uber_data()\n",
    "hourly_weather_data, daily_weather_data, sun_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data\n",
    "\n",
    "When we get the cleaned data, we should store it in a database. The sqlite is a good database for a small project. We can use `sqlalchemy` to create a database and store the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23d31ae",
   "metadata": {},
   "source": [
    "For each dataframe we have, we should create a corresponding table in the database. The data types of tables should be same as the data types of dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOURLY_WEATHER_SCHEMA = \"\"\"CREATE TABLE IF NOT EXISTS hourly_weather (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    DATE DATE,\n",
    "    HourlyWindSpeed FLOAT32,\n",
    "    HourlyPrecipitation FLOAT32\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"CREATE TABLE IF NOT EXISTS daily_weather (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    DATE DATE,\n",
    "    DailyWindSpeed FLOAT32,\n",
    "    DailyPrecipitation FLOAT32\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"CREATE TABLE IF NOT EXISTS taxi_trips (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    pickup_datetime DATE,\n",
    "    pickup_longitude FLOAT32,\n",
    "    pickup_latitude FLOAT32,\n",
    "    dropoff_longitude FLOAT32,\n",
    "    dropoff_latitude FLOAT32,\n",
    "    tip_amount FLOAT32,\n",
    "    distance FLOAT32\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"CREATE TABLE IF NOT EXISTS uber_trips (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    pickup_datetime DATE,\n",
    "    pickup_longitude FLOAT32,\n",
    "    pickup_latitude FLOAT32,\n",
    "    dropoff_longitude FLOAT32,\n",
    "    dropoff_latitude FLOAT32,\n",
    "    distance FLOAT32\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "SUN_DATA_SCHEMA = \"\"\"CREATE TABLE IF NOT EXISTS sun_data (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    DATE DATE,\n",
    "    Sunrise INT32,\n",
    "    Sunset INT32\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8db2ee6",
   "metadata": {},
   "source": [
    "Now, we save the table schema in a file called `schema.sql` for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)\n",
    "    f.write(SUN_DATA_SCHEMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bbf952",
   "metadata": {},
   "source": [
    "Then, we create tables in the database from the schema file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    with open(DATABASE_SCHEMA_FILE, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        query = []\n",
    "        for line in lines:\n",
    "            query.append(line)\n",
    "            # if the line is a semicolon, execute the query\n",
    "            if \";\" in line:\n",
    "                connection.execute(db.text(\"\".join(query)))\n",
    "                query = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database\n",
    "\n",
    "After we create the database, we can add the data to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_weather_data,\n",
    "    \"daily_weather\": daily_weather_data,\n",
    "    \"sun_data\": sun_data\n",
    "}\n",
    "\n",
    "for table, df in map_table_name_to_dataframe.items():\n",
    "    print(\"writing\", table)\n",
    "    df.to_sql(table, engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data\n",
    "\n",
    "After we store the data in the database, we can query the data from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_query_to_file(query: str, outfile: str):\n",
    "    '''Writes the query to the outfile.\n",
    "\n",
    "    Keyword arguments:\n",
    "    query -- The query to write.\n",
    "    outfile -- The name of the file to write to.\n",
    "    '''\n",
    "\n",
    "    with open(QUERY_DIRECTORY + outfile, \"w\") as f:\n",
    "        f.write(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1\n",
    "\n",
    "For 01-2009 through 06-2015, we will find the most popular hour of the day to take a yellow taxi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1 = \"\"\"SELECT strftime('%H', pickup_datetime) AS time, COUNT(*) AS num\n",
    "FROM taxi_trips\n",
    "GROUP BY time\n",
    "ORDER BY num DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, \"most_popular_hour.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d86fd55",
   "metadata": {},
   "source": [
    "### Query 2\n",
    "\n",
    "For 01-2009 through 06-2015, we will find the most popular day of the week to take a uber."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ba2050",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_2 = \"\"\"SELECT strftime('%w', pickup_datetime) AS day, COUNT(*) AS num\n",
    "FROM uber_trips\n",
    "GROUP BY day\n",
    "ORDER BY num DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac95574",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_2).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059e9eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_2, \"most_popular_day.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d555ff11",
   "metadata": {},
   "source": [
    "### Query 3\n",
    "\n",
    "We will find the 95% percentile of distance traveled for all hired trips during July 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a6ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_3 = \"\"\"WITH hired_trips AS (SELECT pickup_datetime,distance \n",
    "          FROM taxi_trips \n",
    "          WHERE pickup_datetime BETWEEN '2013-07-01' AND '2013-08-01'\n",
    "          UNION ALL\n",
    "          SELECT pickup_datetime,distance FROM uber_trips\n",
    "          WHERE pickup_datetime BETWEEN '2013-07-01' AND '2013-08-01')\n",
    "          SELECT distance\n",
    "          FROM hired_trips\n",
    "          ORDER BY distance ASC\n",
    "          LIMIT 1\n",
    "          OFFSET (SELECT COUNT(*) FROM hired_trips) * 95 / 100 - 1 ;\n",
    "          \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff1b6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_3).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603a40c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_3, \"95%_percentile_distance.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941343df",
   "metadata": {},
   "source": [
    "### Query 4\n",
    "\n",
    "We will find the top 10 days with the highest number of hired rides for 2009, and the average distance for each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e957f102",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_4 = \"\"\"WITH hired_trips AS (SELECT pickup_datetime,distance \n",
    "          FROM taxi_trips \n",
    "          WHERE pickup_datetime BETWEEN '2009-01-01' AND '2010-01-01'\n",
    "          UNION ALL\n",
    "          SELECT pickup_datetime,distance FROM uber_trips\n",
    "          WHERE pickup_datetime BETWEEN '2009-01-01' AND '2010-01-01')\n",
    "          SELECT date(pickup_datetime) AS date, AVG(distance) AS avg_distance, COUNT(*) AS num\n",
    "          FROM hired_trips\n",
    "          GROUP BY date\n",
    "          ORDER BY num DESC\n",
    "          LIMIT 10;\n",
    "          \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99665a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_4).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093c3fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_4, \"top_10_days.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2e7515",
   "metadata": {},
   "source": [
    "### Query 5\n",
    "\n",
    "We will find the top 10 windiest days in 2014, and the number of hired trips on those days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947c9a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_5 =\"\"\"SELECT date(pickup_datetime) AS date, COUNT(*) AS num, \n",
    "            FROM (SELECT pickup_datetime FROM taxi_trips\n",
    "            UNION ALL\n",
    "            SELECT pickup_datetime FROM uber_trips)\n",
    "            GROUP BY date\n",
    "            HAVING date IN (SELECT date(DATE) FROM daily_weather WHERE DATE BETWEEN '2014-01-01' AND '2015-01-01' ORDER BY DailyWindSpeed DESC LIMIT 10)\n",
    "         \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f331c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_5).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ade1a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_5, \"top_10_windiest_days.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5ff71a",
   "metadata": {},
   "source": [
    "### Query 6\n",
    "\n",
    "We will find the number of trips taken each hour during the week leading up to Hurricane Sandy(Oct 29-30, 2012), and the hourly precipitation and wind speed during for each hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3bba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_6 = \"\"\"WITH hired_trips AS (SELECT strftime('%Y-%m-%d %H:00:00:00',pickup_datetime) AS DATE\n",
    "            FROM taxi_trips\n",
    "            WHERE pickup_datetime BETWEEN '2012-10-22' AND '2012-10-31'\n",
    "            UNION ALL\n",
    "            SELECT pickup_datetime FROM uber_trips\n",
    "            WHERE pickup_datetime BETWEEN '2012-10-22' AND '2012-10-31')\n",
    "            SELECT strftime('%Y-%m-%d %H:00:00:00',hourly_weather.DATE) AS WDATE, COALESCE(COUNT(hired_trips.DATE),0) AS num, HourlyPrecipitation, HourlyWindSpeed\n",
    "            FROM hourly_weather\n",
    "            LEFT JOIN hired_trips\n",
    "            ON WDATE = hired_trips.DATE\n",
    "            WHERE WDATE BETWEEN '2012-10-22' AND '2012-10-31'\n",
    "            GROUP BY WDATE\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c7d967",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_6).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b02c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_6, \"hurricane_sandy.sql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76289b0c",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data\n",
    "\n",
    "To better display the data, we will visualize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1\n",
    "\n",
    "We will create an animated line chart to show the most popular hour of the day to take a yellow taxi. The histogram can show us the relative popularity of each hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hour_for_taxi_trip():\n",
    "    \"\"\"Plot the number of taxi trips per hour of the day.\"\"\"\n",
    "\n",
    "    df = pd.read_sql_query(QUERY_1, engine).sort_values(by=\"time\", ascending=True)\n",
    "    df.plot(x=\"time\", y=\"num\",kind=\"bar\", title=\"Num of Taxi Trips per Hour\", xlabel=\"Hour\", ylabel=\"Number of Trips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48836f20",
   "metadata": {},
   "source": [
    "As shown below, the most popular hour is 17:00, and the least popular hour is 05:00. Generally speaking, there are more people who take taxis in the evening rush hour, and fewer people take taxis in the middle of the night."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_for_taxi_trip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c785a7ab",
   "metadata": {},
   "source": [
    "### Visualization 2\n",
    "\n",
    "We will create a line chart to show the average distance traveled per month and its 90% confidence interval around the mean. The line chart can display the line and the confidence interval better than other charts. Considering the sample size is small, we will calculate the confidence interval under t-distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7c7b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_distance_per_month():\n",
    "    \"\"\"Plot a line chart to show the average distance traveled per month and its 90% confidence interval around the mean\"\"\"\n",
    "\n",
    "    taxi_query=\"\"\"SELECT strftime('%Y-%m', pickup_datetime) AS year_month, SUM(distance) AS distance\n",
    "                FROM taxi_trips\n",
    "                GROUP BY year_month\n",
    "                \"\"\"\n",
    "\n",
    "    uber_query='''SELECT strftime('%Y-%m', pickup_datetime) AS year_month, SUM(distance) AS distance\n",
    "                FROM uber_trips\n",
    "                GROUP BY year_month\n",
    "                '''\n",
    "\n",
    "    # get the data from the database\n",
    "    taxi_df = pd.read_sql_query(taxi_query, engine)\n",
    "    uber_df = pd.read_sql_query(uber_query, engine)\n",
    "    \n",
    "    # change the data type of the year_month column to datetime\n",
    "    taxi_df['year_month'] = pd.to_datetime(taxi_df['year_month'], format='%Y-%m')\n",
    "    uber_df['year_month'] = pd.to_datetime(uber_df['year_month'], format='%Y-%m')\n",
    "\n",
    "    # calculate the mean, standard error and dof of the distance\n",
    "    taxi_df = taxi_df['distance'].groupby(taxi_df['year_month'].dt.month).agg(['mean', 'count', 'sem'])\n",
    "    uber_df = uber_df['distance'].groupby(uber_df['year_month'].dt.month).agg(['mean', 'count', 'sem'])\n",
    "    taxi_df['dof'] = taxi_df['count'] - 1\n",
    "    uber_df['dof'] = uber_df['count'] - 1\n",
    "\n",
    "    # calculate the 90% confidence interval\n",
    "    taxi_low_CI_bound, taxi_high_CI_bound = stats.t.interval(0.90, taxi_df['dof'], loc=taxi_df['mean'], scale=taxi_df['sem'])\n",
    "    uber_low_CI_bound, uber_high_CI_bound = stats.t.interval(0.90, uber_df['dof'], loc=uber_df['mean'], scale=uber_df['sem'])\n",
    "\n",
    "    # plot the line chart\n",
    "    plt.plot(taxi_df.index, taxi_df['mean'], color='blue', label='Taxi')\n",
    "    plt.plot(uber_df.index, uber_df['mean'], color='red', label='Uber')\n",
    "    plt.fill_between(taxi_df.index, taxi_low_CI_bound, taxi_high_CI_bound, color='blue', alpha=0.2)\n",
    "    plt.fill_between(uber_df.index, uber_low_CI_bound, uber_high_CI_bound, color='red', alpha=0.2)\n",
    "    plt.title('Average Distance Traveled per Month')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Average Distance Traveled')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53acdc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_distance_per_month()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e957bac9",
   "metadata": {},
   "source": [
    "### Visualization 3\n",
    "\n",
    "Define three lat/long coordinate boxes around the three major New York airports: LGA, JFK, and EWR and create a multi-bar chart that compares what day of the week was most popular for drop offs for each airport. The multi-bar chart can show us the most popular day of the week for each airport simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bd79e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropoff_num_for_each_day():\n",
    "    \"\"\"Plot a line chart to show the number of dropoffs for each day.\"\"\"\n",
    "    \n",
    "    LGA_BBOX = [-73.889473,40.766102,-73.857630,40.782806,'LGA']\n",
    "    JFK_BBOX = [-73.826978,40.618945,-73.741319,40.673388,'JFK']\n",
    "    EWR_BBOX = [-74.199343,40.668791,-74.150248,40.712069,'EWR']\n",
    "    df = []\n",
    "    for BBOX in [LGA_BBOX, JFK_BBOX, EWR_BBOX]:\n",
    "        query = f'''WITH hired_trips AS (SELECT pickup_datetime,dropoff_longitude,dropoff_latitude\n",
    "                FROM taxi_trips UNION ALL\n",
    "                SELECT pickup_datetime,dropoff_longitude,dropoff_latitude FROM uber_trips)\n",
    "                SELECT strftime('%w', pickup_datetime) AS day ,COUNT(*) AS {BBOX[4]}\n",
    "                FROM hired_trips\n",
    "                WHERE dropoff_longitude BETWEEN {BBOX[0]} AND {BBOX[2]} AND dropoff_latitude BETWEEN {BBOX[1]} AND {BBOX[3]}\n",
    "                GROUP BY day\n",
    "                '''\n",
    "        df.append(pd.read_sql_query(query, engine))\n",
    "\n",
    "    df[0]['JFK'] = df[1]['JFK']\n",
    "    df[0]['EWR'] = df[2]['EWR']\n",
    "    df[0].plot(x=\"day\", y=[\"LGA\",'JFK','EWR'], kind=\"bar\", title=\"Number of Dropoffs for Each Day for each Airport\", xlabel=\"Day\", ylabel=\"Number of Dropoffs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c11d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropoff_num_for_each_day()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a02f48",
   "metadata": {},
   "source": [
    "### Visualization 4\n",
    "\n",
    "Create a heatmap of all hired trips over a map of the area. The heatmap can show us the density of the hired trips in the area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a6e375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_for_trips():\n",
    "    \"\"\"Make a heatmap of all hired trips over a map of the area.\"\"\"\n",
    "    \n",
    "    query = '''WITH hired_trips AS (SELECT pickup_latitude AS latitude, pickup_longitude AS longitude\n",
    "            FROM taxi_trips UNION ALL\n",
    "            SELECT pickup_latitude AS latitude, pickup_longitude AS longitude FROM uber_trips)\n",
    "            SELECT latitude, longitude\n",
    "            FROM hired_trips\n",
    "            '''\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    \n",
    "    # reindex the GEO\n",
    "    geo_loc = GEO.reset_index()\n",
    "\n",
    "    # make a key from the latitude and longitude\n",
    "    geo_loc['key'] = geo_loc['latitude'] ** 2 +  geo_loc['longitude'] ** 2\n",
    "    df['key'] = df['latitude'] ** 2 +  df['longitude'] ** 2\n",
    "\n",
    "    # sort the key\n",
    "    geo_loc.sort_values(by='key', inplace=True)\n",
    "    df.sort_values(by='key', inplace=True)\n",
    "\n",
    "    # join the two dataframes\n",
    "    df = pd.merge_asof(df, geo_loc, left_on=[\"key\"],right_on=[\"key\"])\n",
    "    df.dropna(subset=[\"LocationID\"], inplace=True)\n",
    "\n",
    "    geo_loc['num'] = df.groupby(\"LocationID\").count()['key']\n",
    "    geo_loc.plot(column='num', cmap='coolwarm', legend=False)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7d98e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_for_trips()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0e1620",
   "metadata": {},
   "source": [
    "### Visualization 5\n",
    "\n",
    "Create a scatter plot that compares tip amount versus distance. A scatter plot can show us the relationship between tip amount and distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b848eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def tip_amount_vs_distance():\n",
    "    \"\"\"Create a scatter plot that compares tip amount versus distance.\"\"\"\n",
    "    \n",
    "    query = '''SELECT tip_amount, distance\n",
    "            FROM taxi_trips\n",
    "            '''\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    \n",
    "    # sample the data to make the scatter plot clearer\n",
    "    df[(df['distance'] < 100) & (df['tip_amount'] > 0)].sample(800).plot(x=\"distance\", y=\"tip_amount\", kind=\"scatter\", title=\"Tip Amount vs Distance\", xlabel=\"Distance\", ylabel=\"Tip Amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d630b5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tip_amount_vs_distance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea579fe",
   "metadata": {},
   "source": [
    "### Visualization 6\n",
    "\n",
    "Create another scatter plot that compares tip amount versus precipitation amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faa73ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tip_amount_vs_precipitation():\n",
    "    \"\"\"Create another scatter plot that compares tip amount versus precipitation amount.\"\"\"\n",
    "    \n",
    "    query1 =\"\"\"SELECT strftime('%Y-%m-%d %H', pickup_datetime) AS DATE, tip_amount\n",
    "            FROM taxi_trips\n",
    "            \"\"\" \n",
    "    query2 =\"\"\"SELECT strftime('%Y-%m-%d %H', DATE) AS DATE, HourlyPrecipitation\n",
    "            FROM hourly_weather\n",
    "            \"\"\"\n",
    "\n",
    "    taxi_data = pd.read_sql_query(query1, engine)\n",
    "    weather_data = pd.read_sql_query(query2, engine)\n",
    "    \n",
    "    taxi_data['DATE'] = pd.to_datetime(taxi_data['DATE'], format='%Y-%m-%d %H')\n",
    "    weather_data['DATE'] = pd.to_datetime(weather_data['DATE'], format='%Y-%m-%d %H')\n",
    "\n",
    "    df = pd.merge(taxi_data, weather_data, on='DATE')\n",
    "    df = df[(df['HourlyPrecipitation'] > 0)]\n",
    "    \n",
    "    # sample the data to make the scatter plot clearer\n",
    "    df.sample(5000).plot(x=\"HourlyPrecipitation\", y=\"tip_amount\", kind=\"scatter\", title=\"Tip Amount vs Precipitation\", xlabel=\"Precipitation\", ylabel=\"Tip Amount\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b5de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tip_amount_vs_precipitation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57664b15",
   "metadata": {},
   "source": [
    "## Matplotlib Animation for Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fd3f572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGdCAYAAADuR1K7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAjBklEQVR4nO3df1CVZf7/8RfIASRDFn9wRDFz100y0w0HxGnGSgR33UnK1WJMjRhdJ6k2XFcp06z9DGs/DFPKaTZr2nJ1dVt3K9dkUcvNIyb2w9/T7laWdEBzEdOEE1zfP/xytpNHxIb7ABfPx4zTcJ/rPue634P5nHNuhjBjjBEAAIAlwtt6AwAAAK2JuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABglYi23kBbaGxsVGVlpS6//HKFhYW19XYAAEALGGN06tQpJSYmKjz8wu/PdMq4qaysVFJSUltvAwAAfA+fffaZ+vXrd8HHO2XcXH755ZLODSc2NraNd9P2fD6fNm/erMzMTLlcrrbejrWYc2gw59BgzqHBnAPV1tYqKSnJ/+/4hXTKuGn6KCo2Npa40bm/PDExMYqNjeUvj4OYc2gw59BgzqHBnIO72C0l3FAMAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCohiZuSkhINGDBA0dHRSktL065du5pdv27dOg0ePFjR0dEaOnSoNm7ceMG1s2bNUlhYmIqLi1t51wAAoCNyPG7Wrl2rgoICLVq0SHv27NGwYcOUlZWl6urqoOt37NihnJwc5eXl6b333lN2drays7O1b9++89b+5S9/0c6dO5WYmOj0ZQAAgA7C8bhZunSpZsyYodzcXF199dVauXKlYmJitGrVqqDrly1bpnHjxmnu3LlKTk7Wo48+quuuu04rVqwIWHf06FHdc889euWVV+RyuZy+DAAA0EFEOPnk9fX1qqioUGFhof9YeHi4MjIy5PF4gp7j8XhUUFAQcCwrK0sbNmzwf93Y2KipU6dq7ty5GjJkyEX3UVdXp7q6Ov/XtbW1kiSfzyefz3cpl2SlphkwC2cx59BgzqHBnEODOQdq6RwcjZvjx4+roaFBCQkJAccTEhJ06NChoOd4vd6g671er//rJUuWKCIiQvfee2+L9lFUVKTFixefd3zz5s2KiYlp0XN0BqWlpW29hU6BOYcGcw4N5hwazPmcM2fOtGido3HjhIqKCi1btkx79uxRWFhYi84pLCwMeDeotrZWSUlJyszMVGxsrFNb7TB8Pp9KS0s1duxYPuJzEHMODeYcGsw5NJhzoKZPXi7G0bjp2bOnunTpoqqqqoDjVVVVcrvdQc9xu93Nrt++fbuqq6vVv39//+MNDQ2aM2eOiouL9cknn5z3nFFRUYqKijrvuMvl4pvlW5hHaDDn0GDOocGcQ4M5n9PSGTh6Q3FkZKRSUlJUVlbmP9bY2KiysjKlp6cHPSc9PT1gvXTu7bim9VOnTtWHH36o999/3/8nMTFRc+fO1ZtvvuncxQAAgA7B8Y+lCgoKNH36dI0YMUKpqakqLi7W6dOnlZubK0maNm2a+vbtq6KiIknSfffdp9GjR+vJJ5/U+PHjtWbNGu3evVvPPfecJKlHjx7q0aNHwGu4XC653W5dddVVTl8OAABo5xyPm9tuu03Hjh3TwoUL5fV6NXz4cG3atMl/0/CRI0cUHv6/N5BGjRql1atXa8GCBXrggQc0aNAgbdiwQddcc43TWwUAABYIyQ3F+fn5ys/PD/rYtm3bzjs2adIkTZo0qcXPH+w+GwAA0Dnxu6UAAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAVglJ3JSUlGjAgAGKjo5WWlqadu3a1ez6devWafDgwYqOjtbQoUO1ceNG/2M+n0/z5s3T0KFDddlllykxMVHTpk1TZWWl05cBAAA6AMfjZu3atSooKNCiRYu0Z88eDRs2TFlZWaqurg66fseOHcrJyVFeXp7ee+89ZWdnKzs7W/v27ZMknTlzRnv27NFDDz2kPXv26NVXX9Xhw4d18803O30pAACgA3A8bpYuXaoZM2YoNzdXV199tVauXKmYmBitWrUq6Pply5Zp3Lhxmjt3rpKTk/Xoo4/quuuu04oVKyRJ3bt3V2lpqSZPnqyrrrpKI0eO1IoVK1RRUaEjR444fTkAAKCdi3Dyyevr61VRUaHCwkL/sfDwcGVkZMjj8QQ9x+PxqKCgIOBYVlaWNmzYcMHXOXnypMLCwhQXFxf08bq6OtXV1fm/rq2tlXTuIy6fz9fCq7FX0wyYhbOYc2gw59BgzqHBnAO1dA6Oxs3x48fV0NCghISEgOMJCQk6dOhQ0HO8Xm/Q9V6vN+j6s2fPat68ecrJyVFsbGzQNUVFRVq8ePF5xzdv3qyYmJiWXEqnUFpa2tZb6BSYc2gw59BgzqHBnM85c+ZMi9Y5GjdO8/l8mjx5sowxevbZZy+4rrCwMODdoNraWiUlJSkzM/OCQdSZ+Hw+lZaWauzYsXK5XG29HWsx59BgzqHBnEODOQdq+uTlYhyNm549e6pLly6qqqoKOF5VVSW32x30HLfb3aL1TWHz6aefasuWLc1GSlRUlKKios477nK5+Gb5FuYRGsw5NJhzaDDn0GDO57R0Bo7eUBwZGamUlBSVlZX5jzU2NqqsrEzp6elBz0lPTw9YL517O+7b65vC5qOPPtI//vEP9ejRw5kLAAAAHY7jH0sVFBRo+vTpGjFihFJTU1VcXKzTp08rNzdXkjRt2jT17dtXRUVFkqT77rtPo0eP1pNPPqnx48drzZo12r17t5577jlJ58LmF7/4hfbs2aPXX39dDQ0N/vtx4uPjFRkZ6fQlAQCAdszxuLntttt07NgxLVy4UF6vV8OHD9emTZv8Nw0fOXJE4eH/ewNp1KhRWr16tRYsWKAHHnhAgwYN0oYNG3TNNddIko4ePaq//e1vkqThw4cHvNbWrVt1ww03OH1JAACgHQvJDcX5+fnKz88P+ti2bdvOOzZp0iRNmjQp6PoBAwbIGNOa2wMAABbhd0sBAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArBKSuCkpKdGAAQMUHR2ttLQ07dq1q9n169at0+DBgxUdHa2hQ4dq48aNAY8bY7Rw4UL16dNHXbt2VUZGhj766CMnLwEAAHQQjsfN2rVrVVBQoEWLFmnPnj0aNmyYsrKyVF1dHXT9jh07lJOTo7y8PL333nvKzs5Wdna29u3b51/z2GOP6emnn9bKlStVXl6uyy67TFlZWTp79qzTlwMAANo5x+Nm6dKlmjFjhnJzc3X11Vdr5cqViomJ0apVq4KuX7ZsmcaNG6e5c+cqOTlZjz76qK677jqtWLFC0rl3bYqLi7VgwQJNmDBB1157rV566SVVVlZqw4YNTl8OAABo5yKcfPL6+npVVFSosLDQfyw8PFwZGRnyeDxBz/F4PCooKAg4lpWV5Q+Xjz/+WF6vVxkZGf7Hu3fvrrS0NHk8Ht1+++3nPWddXZ3q6ur8X9fW1kqSfD6ffD7f974+WzTNgFk4izmHBnMODeYcGsw5UEvn4GjcHD9+XA0NDUpISAg4npCQoEOHDgU9x+v1Bl3v9Xr9jzcdu9Ca7yoqKtLixYvPO75582bFxMS07GI6gdLS0rbeQqfAnEODOYcGcw4N5nzOmTNnWrTO0bhpLwoLCwPeDaqtrVVSUpIyMzMVGxvbhjtrH3w+n0pLSzV27Fi5XK623o61mHNoMOfQYM6hwZwDNX3ycjGOxk3Pnj3VpUsXVVVVBRyvqqqS2+0Oeo7b7W52fdN/q6qq1KdPn4A1w4cPD/qcUVFRioqKOu+4y+Xim+VbmEdoMOfQYM6hwZxDgzmf09IZOHpDcWRkpFJSUlRWVuY/1tjYqLKyMqWnpwc9Jz09PWC9dO7tuKb1V155pdxud8Ca2tpalZeXX/A5AQBA5+H4x1IFBQWaPn26RowYodTUVBUXF+v06dPKzc2VJE2bNk19+/ZVUVGRJOm+++7T6NGj9eSTT2r8+PFas2aNdu/ereeee06SFBYWpl/96lf67W9/q0GDBunKK6/UQw89pMTERGVnZzt9OQAAoJ1zPG5uu+02HTt2TAsXLpTX69Xw4cO1adMm/w3BR44cUXj4/95AGjVqlFavXq0FCxbogQce0KBBg7RhwwZdc801/jW/+c1vdPr0ac2cOVM1NTW6/vrrtWnTJkVHRzt9OQAAoJ0LyQ3F+fn5ys/PD/rYtm3bzjs2adIkTZo06YLPFxYWpkceeUSPPPJIa20RAABYgt8tBQAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALCKY3Fz4sQJTZkyRbGxsYqLi1NeXp6++uqrZs85e/asZs+erR49eqhbt26aOHGiqqqq/I9/8MEHysnJUVJSkrp27ark5GQtW7bMqUsAAAAdkGNxM2XKFO3fv1+lpaV6/fXX9fbbb2vmzJnNnnP//ffrtdde07p16/TWW2+psrJSt956q//xiooK9e7dWy+//LL279+vBx98UIWFhVqxYoVTlwEAADqYCCee9ODBg9q0aZPeffddjRgxQpK0fPly/exnP9MTTzyhxMTE8845efKknn/+ea1evVo33XSTJOmFF15QcnKydu7cqZEjR+quu+4KOGfgwIHyeDx69dVXlZ+f78SlAACADsaRd248Ho/i4uL8YSNJGRkZCg8PV3l5edBzKioq5PP5lJGR4T82ePBg9e/fXx6P54KvdfLkScXHx7fe5gEAQIfmyDs3Xq9XvXv3DnyhiAjFx8fL6/Ve8JzIyEjFxcUFHE9ISLjgOTt27NDatWv1xhtvNLufuro61dXV+b+ura2VJPl8Pvl8votdjvWaZsAsnMWcQ4M5hwZzDg3mHKilc7ikuJk/f76WLFnS7JqDBw9eylN+b/v27dOECRO0aNEiZWZmNru2qKhIixcvPu/45s2bFRMT49QWO5zS0tK23kKnwJxDgzmHBnMODeZ8zpkzZ1q07pLiZs6cObrzzjubXTNw4EC53W5VV1cHHP/mm2904sQJud3uoOe53W7V19erpqYm4N2bqqqq8845cOCAxowZo5kzZ2rBggUX3XdhYaEKCgr8X9fW1iopKUmZmZmKjY296Pm28/l8Ki0t1dixY+Vyudp6O9ZizqHBnEODOYcGcw7U9MnLxVxS3PTq1Uu9evW66Lr09HTV1NSooqJCKSkpkqQtW7aosbFRaWlpQc9JSUmRy+VSWVmZJk6cKEk6fPiwjhw5ovT0dP+6/fv366abbtL06dP1f//3fy3ad1RUlKKios477nK5+Gb5FuYRGsw5NJhzaDDn0GDO57R0Bo7cUJycnKxx48ZpxowZ2rVrl9555x3l5+fr9ttv9/+k1NGjRzV48GDt2rVLktS9e3fl5eWpoKBAW7duVUVFhXJzc5Wenq6RI0dKOvdR1I033qjMzEwVFBTI6/XK6/Xq2LFjTlwGAADogBy5oViSXnnlFeXn52vMmDEKDw/XxIkT9fTTT/sf9/l8Onz4cMDnZ0899ZR/bV1dnbKysvTMM8/4H1+/fr2OHTuml19+WS+//LL/+BVXXKFPPvnEqUsBAAAdiGNxEx8fr9WrV1/w8QEDBsgYE3AsOjpaJSUlKikpCXrOww8/rIcffrg1twkAACzD75YCAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAVnEsbk6cOKEpU6YoNjZWcXFxysvL01dffdXsOWfPntXs2bPVo0cPdevWTRMnTlRVVVXQtV9++aX69eunsLAw1dTUOHAFAACgI3IsbqZMmaL9+/ertLRUr7/+ut5++23NnDmz2XPuv/9+vfbaa1q3bp3eeustVVZW6tZbbw26Ni8vT9dee60TWwcAAB2YI3Fz8OBBbdq0Sb///e+Vlpam66+/XsuXL9eaNWtUWVkZ9JyTJ0/q+eef19KlS3XTTTcpJSVFL7zwgnbs2KGdO3cGrH322WdVU1OjX//6105sHwAAdGARTjypx+NRXFycRowY4T+WkZGh8PBwlZeX65ZbbjnvnIqKCvl8PmVkZPiPDR48WP3795fH49HIkSMlSQcOHNAjjzyi8vJy/ec//2nRfurq6lRXV+f/ura2VpLk8/nk8/m+1zXapGkGzMJZzDk0mHNoMOfQYM6BWjoHR+LG6/Wqd+/egS8UEaH4+Hh5vd4LnhMZGam4uLiA4wkJCf5z6urqlJOTo8cff1z9+/dvcdwUFRVp8eLF5x3fvHmzYmJiWvQcnUFpaWlbb6FTYM6hwZxDgzmHBnM+58yZMy1ad0lxM3/+fC1ZsqTZNQcPHryUp7wkhYWFSk5O1h133HHJ5xUUFPi/rq2tVVJSkjIzMxUbG9va2+xwfD6fSktLNXbsWLlcrrbejrWYc2gw59BgzqHBnAM1ffJyMZcUN3PmzNGdd97Z7JqBAwfK7Xaruro64Pg333yjEydOyO12Bz3P7Xarvr5eNTU1Ae/eVFVV+c/ZsmWL9u7dq/Xr10uSjDGSpJ49e+rBBx8M+u6MJEVFRSkqKuq84y6Xi2+Wb2EeocGcQ4M5hwZzDg3mfE5LZ3BJcdOrVy/16tXrouvS09NVU1OjiooKpaSkSDoXJo2NjUpLSwt6TkpKilwul8rKyjRx4kRJ0uHDh3XkyBGlp6dLkv785z/r66+/9p/z7rvv6q677tL27dv1wx/+8FIuBQAAWMqRe26Sk5M1btw4zZgxQytXrpTP51N+fr5uv/12JSYmSpKOHj2qMWPG6KWXXlJqaqq6d++uvLw8FRQUKD4+XrGxsbrnnnuUnp7uv5n4uwFz/Phx/+t9914dAADQOTkSN5L0yiuvKD8/X2PGjFF4eLgmTpyop59+2v+4z+fT4cOHA24Oeuqpp/xr6+rqlJWVpWeeecapLQIAAAs5Fjfx8fFavXr1BR8fMGCA/56ZJtHR0SopKVFJSUmLXuOGG2447zkAAEDnxu+WAgAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBVItp6A23BGCNJqq2tbeOdtA8+n09nzpxRbW2tXC5XW2/HWsw5NJhzaDDn0GDOgZr+3W76d/xCOmXcnDp1SpKUlJTUxjsBAACX6tSpU+revfsFHw8zF8sfCzU2NqqyslKXX365wsLC2no7ba62tlZJSUn67LPPFBsb29bbsRZzDg3mHBrMOTSYcyBjjE6dOqXExESFh1/4zppO+c5NeHi4+vXr19bbaHdiY2P5yxMCzDk0mHNoMOfQYM7/09w7Nk24oRgAAFiFuAEAAFYhbqCoqCgtWrRIUVFRbb0VqzHn0GDOocGcQ4M5fz+d8oZiAABgL965AQAAViFuAACAVYgbAABgFeIGAABYhbjpBE6cOKEpU6YoNjZWcXFxysvL01dffdXsOWfPntXs2bPVo0cPdevWTRMnTlRVVVXQtV9++aX69eunsLAw1dTUOHAFHYMTc/7ggw+Uk5OjpKQkde3aVcnJyVq2bJnTl9LulJSUaMCAAYqOjlZaWpp27drV7Pp169Zp8ODBio6O1tChQ7Vx48aAx40xWrhwofr06aOuXbsqIyNDH330kZOX0CG05px9Pp/mzZunoUOH6rLLLlNiYqKmTZumyspKpy+j3Wvt7+dvmzVrlsLCwlRcXNzKu+5gDKw3btw4M2zYMLNz506zfft286Mf/cjk5OQ0e86sWbNMUlKSKSsrM7t37zYjR440o0aNCrp2woQJ5qc//amRZP773/86cAUdgxNzfv755829995rtm3bZv7973+bP/zhD6Zr165m+fLlTl9Ou7FmzRoTGRlpVq1aZfbv329mzJhh4uLiTFVVVdD177zzjunSpYt57LHHzIEDB8yCBQuMy+Uye/fu9a/53e9+Z7p37242bNhgPvjgA3PzzTebK6+80nz99dehuqx2p7XnXFNTYzIyMszatWvNoUOHjMfjMampqSYlJSWUl9XuOPH93OTVV181w4YNM4mJieapp55y+EraN+LGcgcOHDCSzLvvvus/9ve//92EhYWZo0ePBj2npqbGuFwus27dOv+xgwcPGknG4/EErH3mmWfM6NGjTVlZWaeOG6fn/G133323ufHGG1tv8+1camqqmT17tv/rhoYGk5iYaIqKioKunzx5shk/fnzAsbS0NPPLX/7SGGNMY2Ojcbvd5vHHH/c/XlNTY6Kioswf//hHB66gY2jtOQeza9cuI8l8+umnrbPpDsipOX/++eemb9++Zt++feaKK67o9HHDx1KW83g8iouL04gRI/zHMjIyFB4ervLy8qDnVFRUyOfzKSMjw39s8ODB6t+/vzwej//YgQMH9Mgjj+ill15q9heYdQZOzvm7Tp48qfj4+NbbfDtWX1+vioqKgBmFh4crIyPjgjPyeDwB6yUpKyvLv/7jjz+W1+sNWNO9e3elpaU1O3ebOTHnYE6ePKmwsDDFxcW1yr47Gqfm3NjYqKlTp2ru3LkaMmSIM5vvYDr3v0idgNfrVe/evQOORUREKD4+Xl6v94LnREZGnvc/oISEBP85dXV1ysnJ0eOPP67+/fs7sveOxKk5f9eOHTu0du1azZw5s1X23d4dP35cDQ0NSkhICDje3Iy8Xm+z65v+eynPaTsn5vxdZ8+e1bx585STk9NpfwGkU3NesmSJIiIidO+997b+pjso4qaDmj9/vsLCwpr9c+jQIcdev7CwUMnJybrjjjsce432oK3n/G379u3ThAkTtGjRImVmZobkNYHW4PP5NHnyZBlj9Oyzz7b1dqxSUVGhZcuW6cUXX1RYWFhbb6fdiGjrDeD7mTNnju68885m1wwcOFBut1vV1dUBx7/55hudOHFCbrc76Hlut1v19fWqqakJeFehqqrKf86WLVu0d+9erV+/XtK5nz6RpJ49e+rBBx/U4sWLv+eVtS9tPecmBw4c0JgxYzRz5kwtWLDge11LR9SzZ0916dLlvJ/UCzajJm63u9n1Tf+tqqpSnz59AtYMHz68FXffcTgx5yZNYfPpp59qy5YtnfZdG8mZOW/fvl3V1dUB76A3NDRozpw5Ki4u1ieffNK6F9FRtPVNP3BW042uu3fv9h978803W3Sj6/r16/3HDh06FHCj67/+9S+zd+9e/59Vq1YZSWbHjh0XvOvfZk7N2Rhj9u3bZ3r37m3mzp3r3AW0Y6mpqSY/P9//dUNDg+nbt2+zN2D+/Oc/DziWnp5+3g3FTzzxhP/xkydPckNxK8/ZGGPq6+tNdna2GTJkiKmurnZm4x1Ma8/5+PHjAf8v3rt3r0lMTDTz5s0zhw4dcu5C2jniphMYN26c+clPfmLKy8vNP//5TzNo0KCAH1H+/PPPzVVXXWXKy8v9x2bNmmX69+9vtmzZYnbv3m3S09NNenr6BV9j69atnfqnpYxxZs579+41vXr1MnfccYf54osv/H860z8Ua9asMVFRUebFF180Bw4cMDNnzjRxcXHG6/UaY4yZOnWqmT9/vn/9O++8YyIiIswTTzxhDh48aBYtWhT0R8Hj4uLMX//6V/Phhx+aCRMm8KPgrTzn+vp6c/PNN5t+/fqZ999/P+D7t66urk2usT1w4vv5u/hpKeKmU/jyyy9NTk6O6datm4mNjTW5ubnm1KlT/sc//vhjI8ls3brVf+zrr782d999t/nBD35gYmJizC233GK++OKLC74GcePMnBctWmQknffniiuuCOGVtb3ly5eb/v37m8jISJOammp27tzpf2z06NFm+vTpAev/9Kc/mR//+McmMjLSDBkyxLzxxhsBjzc2NpqHHnrIJCQkmKioKDNmzBhz+PDhUFxKu9aac276fg/259t/Bzqj1v5+/i7ixpgwY/7/zRIAAAAW4KelAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAVvl/wi4AUnn1LKIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read the data\n",
    "df = pd.read_sql_query(QUERY_1, engine).sort_values(by=\"time\", ascending=True)\n",
    "df.index = df['time']\n",
    "\n",
    "def data_gen():\n",
    "    \"\"\"Generate data for the plot.\"\"\"\n",
    "\n",
    "    for cnt in range(24):\n",
    "        yield int(df['time'][cnt]), int(df['num'][cnt])\n",
    "\n",
    "# init the plot\n",
    "fig, ax = plt.subplots()\n",
    "line, = ax.plot([], [], lw=2)\n",
    "ax.grid()\n",
    "xdata, ydata = [], []\n",
    "\n",
    "def init():\n",
    "    \"\"\"Initialize the plot.\"\"\"\n",
    "\n",
    "    ax.set_ylim(0, 14000)\n",
    "    ax.set_xlim(0, 23)\n",
    "    ax.set_xlabel('Hour')\n",
    "    ax.set_ylabel('Number of Trips')\n",
    "    ax.set_title('Number of Trips for Each Hour of the Day')\n",
    "    del xdata[:]\n",
    "    del ydata[:]\n",
    "    line.set_data(xdata, ydata)\n",
    "    return line,\n",
    "\n",
    "def run(data: tuple):\n",
    "    \"\"\"Update the plot.\"\"\"\n",
    "\n",
    "    x, y = data\n",
    "    xdata.append(x)\n",
    "    ydata.append(y)\n",
    "    line.set_data(xdata, ydata)\n",
    "\n",
    "    return line,\n",
    "\n",
    "ani = animation.FuncAnimation(fig, run, data_gen, interval=23,\n",
    "                            repeat=False, init_func=init)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78cd982",
   "metadata": {},
   "source": [
    "## Unit Tests\n",
    "\n",
    "We will write unit tests for the functions we defined in the project. Unit tests can help us find bugs in our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100585be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestNotebook(unittest.TestCase):\n",
    "    \"\"\"Tests for each function in the notebook.\"\"\"\n",
    "\n",
    "    def test_calculate_distance(self):\n",
    "        \"\"\"Test the calculate_distance function.\"\"\"\n",
    "\n",
    "        from_coord = pd.DataFrame({'pickup_latitude': [40.738353, 40.728225, 40.74077], 'pickup_longitude': [-73.999816, -73.994355, -74.005043]})\n",
    "        to_coord = pd.DataFrame({'dropoff_latitude': [40.723217, 40.750325, 40.772647], 'dropoff_longitude': [-73.999511, -73.99471, -73.962565]})\n",
    "        distance = pd.Series([1.683, 2.457, 5.036]) # get the distance from https://www.hhlink.com/%E7%BB%8F%E7%BA%AC%E5%BA%A6\n",
    "\n",
    "        cal_distance = calculate_distance(from_coord, to_coord)\n",
    "\n",
    "        assert np.allclose(cal_distance, distance, atol=1e-03) == True\n",
    "    \n",
    "    def test_add_distance_column(self):\n",
    "        \"\"\"Test the add_distance_column function.\"\"\"\n",
    "\n",
    "        df = pd.DataFrame({'pickup_latitude': [40.738353, 40.728225, 40.74077], 'pickup_longitude': [-73.999816, -73.994355, -74.005043], 'dropoff_latitude': [40.723217, 40.750325, 40.772647], 'dropoff_longitude': [-73.999511, -73.99471, -73.962565]})\n",
    "        df = add_distance_column(df)\n",
    "\n",
    "        assert df['distance'].isnull().values.any() == False\n",
    "    \n",
    "    def test_find_taxi_parquet_urls(self):\n",
    "        \"\"\"Test the find_taxi_parquet_urls function.\"\"\"\n",
    "\n",
    "        urls = find_taxi_parquet_urls()\n",
    "        assert len(urls) == 78\n",
    "        assert urls[0] == 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-01.parquet'\n",
    "    \n",
    "    def test_get_and_clean_month_taxi_data(self):\n",
    "        \"\"\"Test the get_and_clean_month_taxi_data function.\"\"\"\n",
    "\n",
    "        url = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-01.parquet'\n",
    "        df = get_and_clean_month_taxi_data(url)\n",
    "\n",
    "        assert df.shape[0] < 3000\n",
    "        assert df.notnull().values.any() == True\n",
    "        assert list(df.columns) == ['pickup_datetime', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'tip_amount']\n",
    "        assert df.dtypes['pickup_datetime'] == 'datetime64[ns]'\n",
    "        assert df.dtypes['pickup_longitude'] == 'float32'\n",
    "        assert df.dtypes['pickup_latitude'] == 'float32'\n",
    "        assert df.dtypes['dropoff_longitude'] == 'float32'\n",
    "        assert df.dtypes['dropoff_latitude'] == 'float32'\n",
    "        assert df.dtypes['tip_amount'] == 'float32'\n",
    "    \n",
    "    def test_get_and_clean_taxi_data(self):\n",
    "        \"\"\"Test the get_and_clean_taxi_data function.\"\"\"\n",
    "\n",
    "        urls = ['https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-01.parquet','https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-01.parquet']\n",
    "        df = get_and_clean_taxi_data(urls)\n",
    "        print(df.columns)\n",
    "\n",
    "        assert df.shape[0] > 4000\n",
    "        assert df.notnull().values.any() == True\n",
    "        assert list(df.columns) == ['pickup_datetime', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'tip_amount', 'distance']\n",
    "        assert df.dtypes['pickup_datetime'] == 'datetime64[ns]'\n",
    "        assert df.dtypes['pickup_longitude'] == 'float32'\n",
    "        assert df.dtypes['pickup_latitude'] == 'float32'\n",
    "        assert df.dtypes['dropoff_longitude'] == 'float32'\n",
    "        assert df.dtypes['dropoff_latitude'] == 'float32'\n",
    "        assert df.dtypes['tip_amount'] == 'float32'\n",
    "        assert df.dtypes['distance'] == 'float32'\n",
    "    \n",
    "    def test_load_and_clean_uber_data(self):\n",
    "        \"\"\"Test the load_and_clean_uber_data function.\"\"\"\n",
    "\n",
    "        df = load_and_clean_uber_data()\n",
    "\n",
    "        assert df.shape == (195472, 6)\n",
    "        assert list(df.columns) == ['pickup_datetime', 'pickup_longitude', 'pickup_latitude',\n",
    "       'dropoff_longitude', 'dropoff_latitude', 'distance']\n",
    "\n",
    "    def test_clean_month_weather_data_hourly(self):\n",
    "        \"\"\"Test the clean_month_weather_data_hourly function.\"\"\"\n",
    "\n",
    "        csv_file = '2009_weather.csv'\n",
    "        df = clean_month_weather_data_hourly(csv_file)\n",
    "\n",
    "        assert list(df.columns) == ['DATE', 'HourlyWindSpeed', 'HourlyPrecipitation']\n",
    "        assert df.dtypes['DATE'] == 'datetime64[ns]'\n",
    "        assert df.dtypes['HourlyWindSpeed'] == 'float32'\n",
    "        assert df.dtypes['HourlyPrecipitation'] == 'float32'\n",
    "\n",
    "    def test_clean_month_weather_data_daily(self):\n",
    "        \"\"\"Test the clean_month_weather_data_daily function.\"\"\"\n",
    "\n",
    "        csv_file = '2009_weather.csv'\n",
    "        df, df_sun = clean_month_weather_data_daily(csv_file)\n",
    "\n",
    "        assert list(df.columns) == ['DATE', 'DailyWindSpeed', 'DailyPrecipitation']\n",
    "        assert df.dtypes['DATE'] == 'datetime64[ns]'\n",
    "        assert df.dtypes['DailyWindSpeed'] == 'float32'\n",
    "        assert df.dtypes['DailyPrecipitation'] == 'float32'\n",
    "        assert list(df_sun.columns) == ['DATE', 'Sunrise', 'Sunset']\n",
    "        assert df_sun.dtypes['DATE'] == 'datetime64[ns]'\n",
    "        assert df_sun.dtypes['Sunset'] == 'int32'\n",
    "        assert df_sun.dtypes['Sunrise'] == 'int32'\n",
    "\n",
    "    def test_load_and_clean_weather_data(self):\n",
    "        \"\"\"Test the load_and_clean_weather_data function.\"\"\"\n",
    "\n",
    "        hourly_data, daily_data, sun_data = load_and_clean_weather_data()\n",
    "\n",
    "        assert list(hourly_data.columns) == ['DATE', 'HourlyWindSpeed', 'HourlyPrecipitation']\n",
    "        assert list(daily_data.columns) == ['DATE', 'DailyWindSpeed', 'DailyPrecipitation']\n",
    "        assert list(sun_data.columns) == ['DATE', 'Sunrise', 'Sunset']\n",
    "\n",
    "    def test_write_query_to_file(self):\n",
    "        \"\"\"Test the write_query_to_file function.\"\"\"\n",
    "\n",
    "        import tempfile\n",
    "        outfile_path = tempfile.mkstemp()[1]\n",
    "\n",
    "        query = 'SELECT * FROM taxi_data'\n",
    "        write_query_to_file(query, outfile_path)\n",
    "\n",
    "        with open(outfile_path, 'r') as f:\n",
    "            query = f.read()\n",
    "        assert query == 'SELECT * FROM taxi_data'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcbdc57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc50670726e829d05772f0a8941686a10215a3e79c3aeb954cea988b6ae2af0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
